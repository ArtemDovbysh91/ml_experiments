# ml_experiments
This repository contains a collection of machine learning experiments implemented using the fast.ai library, which is based on the PyTorch framework. Each experiment is a combination of concepts and techniques from the courses available at the https://course.fast.ai/.

## Experiment 1: Beer or Bear?
Experiment 1 aims to distinguish between images containing beers and bears. While this might sound like a fun and quirky experiment, it serves as a foundation for understanding how to create an image classifier using Fast.ai.

This experiment has a Kaggle link with a cloud equivalent [Kaggle](https://www.kaggle.com/code/artemdovbysh/exp-1-beer-or-bear)

## Experiment 2: ML Model for Image Classification (Dogs vs Cats), practical example.
This experiment involves the creation and utilization of a ml model for image classification, specifically distinguishing between images of dogs and cats. The project consists of two main components: a Jupyter Notebook file for generating the model and another file for using the generated model. The "Using Model.ipynb" shows how to utilize the trained model for making predictions on new images. There are some instructions on how to run the local instance of the Gradio site. This site is also available on [Huggingface](https://huggingface.co/spaces/ArtemDovbysh/minimal). The [Keggle link](https://www.kaggle.com/code/artemdovbysh/saving-a-basic-fastai-model).

## Experiment 3: How does a neural net really work
Experiment 3 kicks off by delving into the fundamentals of gradient descent and its intertwining with machine learning([Keggle](https://www.kaggle.com/code/artemdovbysh/how-does-a-neural-net-really-work)). Additionally, there's a segment dedicated to evaluating the top-performing image models in machine learning([Keggle](https://www.kaggle.com/code/artemdovbysh/which-image-models-are-best)).

## Experiment 4: Getting started with NLP for absolute beginners
The process starts with importing the data. Next, it moves on to Tokenization and numericalization. Following that, it readies the test dataset to assess the training quality. This is succeeded by training, measuring prediction accuracy, and finally, evaluating the results.
